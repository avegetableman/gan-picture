import torch.nn as nn
import torch
from attention import Attention
class pbq(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv=nn.Conv2d(3,64,4,2)
        self.conv1=nn.Conv2d(64,128,4,2,1)
        self.conv2=nn.Conv2d(128,256,4,2,1)
        self.conv3=nn.Conv2d(256,512,4,2,1)
        self.conv4=nn.Conv2d(512,1024,4,2,1)
        self.conv5=nn.Conv2d(1024,1,4,1)
        self.bn4=nn.BatchNorm2d(1024)
        self.bn3=nn.BatchNorm2d(512)
        self.bn2=nn.BatchNorm2d(256)
        self.bn1=nn.BatchNorm2d(128)
        self.bn=nn.BatchNorm2d(64)
        self.lk=nn.LeakyReLU(0.2)
    def forward(self,x):
        x=self.conv(x)
        x=self.bn(x)
        x=self.lk(x)
        x=self.conv1(x)
        x=self.bn1(x)
        x=self.lk(x)
        x=self.conv2(x)
        x=self.bn2(x)
        x=self.lk(x)
        x=self.conv3(x)
        x=self.bn3(x)
        x=self.lk(x)
        x=self.conv4(x)
        x=self.bn4(x)
        x=self.lk(x)
        x=self.conv5(x)
        return x

class pbq_with_attention(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv=nn.Conv2d(3,64,4,2)
        self.atten = Attention(64)
        self.conv1=nn.Conv2d(64,128,4,2,1)
        self.atten1 = Attention(128)
        self.conv2=nn.Conv2d(128,256,4,2,1)
        self.atten2 = Attention(256)
        self.conv3=nn.Conv2d(256,512,4,2,1)
        self.atten3 = Attention(512)
        self.conv4=nn.Conv2d(512,1024,4,2,1)
        self.atten4 = Attention(1024)
        self.conv5=nn.Conv2d(1024,1,4,1)
        self.bn4=nn.BatchNorm2d(1024)
        self.bn3=nn.BatchNorm2d(512)
        self.bn2=nn.BatchNorm2d(256)
        self.bn1=nn.BatchNorm2d(128)
        self.bn=nn.BatchNorm2d(64)
        self.lk=nn.LeakyReLU(0.2)
    def forward(self,x):
        x=self.conv(x)
        x=self.bn(x)
        x=self.atten(x)
        x=self.lk(x)
        x=self.conv1(x)
        x=self.bn1(x)
        x=self.atten1(x)
        x=self.lk(x)
        x=self.conv2(x)
        x=self.bn2(x)
        x=self.atten2(x)
        x=self.lk(x)
        x=self.conv3(x)
        x=self.bn3(x)
        x=self.atten3(x)
        x=self.lk(x)
        x=self.conv4(x)
        x=self.bn4(x)
        x=self.atten4(x)
        x=self.lk(x)
        x=self.conv5(x)
        return x

if __name__ == "__main__":
    s=pbq_with_attention()
    seed=s(torch.randn(11,3,130,130))
    print(seed.view(11,-1).shape)