import torch.nn as nn
import torch
from attention import Attention
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv=nn.ConvTranspose2d(100,1024,4,1)
        self.conv1=nn.ConvTranspose2d(1024,512,4,2,1)
        self.conv2=nn.ConvTranspose2d(512,256,4,2,1)
        self.conv3=nn.ConvTranspose2d(256,128,4,2,1)
        self.conv4=nn.ConvTranspose2d(128,64,4,2,1)
        self.conv5=nn.ConvTranspose2d(64,3,4,2)
        self.bn=nn.BatchNorm2d(1024)
        self.bn1=nn.BatchNorm2d(512)
        self.bn2=nn.BatchNorm2d(256)
        self.bn3=nn.BatchNorm2d(128)
        self.bn4=nn.BatchNorm2d(64)
        self.lk=nn.LeakyReLU(0.2)
        self.tanh=nn.Tanh()
        self.drop=nn.Dropout2d(0.5)
    def forward(self,x):
        x=self.conv(x)
        x=self.bn(x)
        x=self.lk(x)
        x=self.conv1(x)
        x=self.bn1(x)
        x=self.lk(x)
        x=self.conv2(x)
        x=self.bn2(x)
        x=self.lk(x)
        x=self.conv3(x)
        x=self.bn3(x)
        x=self.lk(x)
        x=self.conv4(x)
        x=self.bn4(x)
        x=self.lk(x)
        #x=self.drop(x)
        x=self.conv5(x)
        x=self.tanh(x)
        return x
    
class Net_with_attention(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv=nn.ConvTranspose2d(100,1024,4,1)
        self.atten = Attention(1024)
        self.conv1=nn.ConvTranspose2d(1024,512,4,2,1)
        self.atten1 = Attention(512)
        self.conv2=nn.ConvTranspose2d(512,256,4,2,1)
        self.atten2 = Attention(256)
        self.conv3=nn.ConvTranspose2d(256,128,4,2,1)
        self.atten3 = Attention(128)
        self.conv4=nn.ConvTranspose2d(128,64,4,2,1)
        self.atten4 = Attention(64)
        self.conv5=nn.ConvTranspose2d(64,3,4,2)
        self.bn=nn.BatchNorm2d(1024)
        self.bn1=nn.BatchNorm2d(512)
        self.bn2=nn.BatchNorm2d(256)
        self.bn3=nn.BatchNorm2d(128)
        self.bn4=nn.BatchNorm2d(64)
        self.lk=nn.LeakyReLU(0.2)
        self.tanh=nn.Tanh()
        self.drop=nn.Dropout2d(0.5)
    def forward(self,x):
        x=self.conv(x)
        x=self.bn(x)
        x=self.atten(x)
        x=self.lk(x)
        x=self.conv1(x)
        x=self.bn1(x)
        x=self.atten1(x)
        x=self.lk(x)
        x=self.conv2(x)
        x=self.bn2(x)
        x=self.atten2(x)
        x=self.lk(x)
        x=self.conv3(x)
        x=self.bn3(x)
        x=self.atten3(x)
        x=self.lk(x)
        x=self.conv4(x)
        x=self.bn4(x)
        x=self.atten4(x)
        x=self.lk(x)
        x=self.conv5(x)
        x=self.tanh(x)
        return x

if __name__ == "__main__":
    s=Net_with_attention()
    seed=s(torch.randn(1,100,1,1))
    print(seed.shape)